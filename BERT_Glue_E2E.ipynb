{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Glue E2E.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CAIt_MzQypn"
      },
      "source": [
        "## BERT Glue E2E\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQx5baTLQ8Dg"
      },
      "source": [
        "# You will use a separate model to preprocess text before using it to fine-tune BERT. \n",
        "# This model depends on tensorflow/text, which you will install below.\n",
        "% pip install -q -U tensorflow-text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz4FkliSRRML"
      },
      "source": [
        "# You will use the AdamW optimizer from tensorflow/models to fine-tune BERT, which you will install as well.\n",
        "% pip install -q -U tf-models-official"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kui1rBA8RhlD",
        "outputId": "c6d4c407-3b0f-4a28-e0c0-7427fc77667d"
      },
      "source": [
        "% pip install -U tfds-nightly"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tfds-nightly in /usr/local/lib/python3.6/dist-packages (4.2.0.dev202102150106)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.27.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.2 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.2->tfds-nightly) (53.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6otXGIwRmKB"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_XLCnWNRpmX"
      },
      "source": [
        "# The TPU can only read directly from Cloud Storage buckets.\n",
        "# The following setting is automatic in Colab\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xdhujktSc1W"
      },
      "source": [
        "**Connect to the TPU worker**\n",
        "\n",
        "The following code connects to the TPU worker and changes TensorFlow's default device to the CPU device on the TPU worker. It also defines a TPU distribution strategy that you will use to distribute model training onto the 8 separate TPU cores available on this one TPU worker. See TensorFlow's TPU guide for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxzvcVhZSRqh",
        "outputId": "ec68ae9c-84c4-4965-ec46-98f1ee1f1cbc"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.environ['COLAB_TPU_ADDR']:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.test.is_gpu_available():\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recomended.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NocZavx5Txr5"
      },
      "source": [
        "Choose a BERT model to fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_va1MnFSqR-",
        "outputId": "37455944-a66b-49f3-89fe-d62e65c83969"
      },
      "source": [
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' \n",
        "bert_model_name = 'bert_en_wwm_uncased_L-24_H-1024_A-16'\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz7hBGP7UG8B"
      },
      "source": [
        "Preprocess the Text\n",
        "\n",
        "On the Classify text with BERT colab the preprocessing model is used directly embedded with the BERT encoder.\n",
        "\n",
        "This tutorial demonstrates how to do preprocessing as part of your input pipeline for training, using Dataset.map, and then merge it into the model that gets exported for inference. That way, both training and inference can work from raw text inputs, although the TPU itself requires numeric inputs.\n",
        "\n",
        "TPU requirements aside, it can help performance have preprocessing done asynchronously in an input pipeline (you can learn more in the tf.data performance guide).\n",
        "\n",
        "This tutorial also demonstrates how to build multi-input models, and how to adjust the sequence length of the inputs to BERT.\n",
        "\n",
        "Let's demonstrate the preprocessing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlX1C9xXT0qk",
        "outputId": "df017017-2e78-4559-c657-d33725a22f55"
      },
      "source": [
        "bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "tok = bert_preprocess.tokenize(tf.constant(['Hello TensorFlow!']))\n",
        "print(tok)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[[7592], [23435, 12314], [999]]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hIqr3DXUqMk"
      },
      "source": [
        "Each preprocessing model also provides a method,.bert_pack_inputs(tensors, seq_length), which takes a list of tokens (like tok above) and a sequence length argument. This packs the inputs to create a dictionary of tensors in the format expected by the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z--NzLQUPub",
        "outputId": "2cae77f1-d69f-4e29-dccc-8b05d613c135"
      },
      "source": [
        "text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))\n",
        "\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape Word Ids :  (1, 20)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[  101  7592 23435 12314   999   102  7592 23435 12314   999   102     0\n",
            "     0     0     0     0], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 20)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 20)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h894n58YV1rk"
      },
      "source": [
        "Here are some details to pay attention to:\n",
        "\n",
        "input_mask The mask allows the model to cleanly differentiate between the content and the padding. The mask has the same shape as the input_word_ids, and contains a 1 anywhere the input_word_ids is not padding.\n",
        "input_type_ids has the same shape of input_mask, but inside the non-padded region, contains a 0 or a 1 indicating which sentence the token is a part of.\n",
        "Next, you will create a preprocessing model that encapsulates all this logic. Your model will take strings as input, and return appropriately formatted objects which can be passed to BERT.\n",
        "\n",
        "Each BERT model has a specific preprocessing model, make sure to use the proper one described on the BERT's model documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLVOfYEzUs3R"
      },
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wpFi8OX5pv"
      },
      "source": [
        "Let's demonstrate the preprocessing model. You will create a test with two sentences input (input1 and input2). The output is what a BERT model would expect as input: input_word_ids, input_masks and input_type_ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkC-QmI4Xyvj",
        "outputId": "a89e08ce-959a-4891-884d-bb8865f26565"
      },
      "source": [
        "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\n",
        "test_text = [np.array(['some random test sentence']),\n",
        "             np.array(['another sentence'])]\n",
        "text_preprocessed = test_preprocess_model(test_text)\n",
        "\n",
        "print('Keys           : ', list(text_preprocessed.keys()))\n",
        "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
        "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
        "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
        "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
        "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
        "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys           :  ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape Word Ids :  (1, 128)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
            "    0    0], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (1, 128)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (1, 128)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "sjmxE7JQX81W",
        "outputId": "b556c9c6-b764-4f67-aa27-41e10a4a1077"
      },
      "source": [
        "tf.keras.utils.plot_model(test_preprocess_model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apW-aF_FYWEn"
      },
      "source": [
        "# To apply the preprocessing in all the inputs from the dataset, you will use the map function from the dataset. \n",
        "# The result is then cached for performance.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_dataset_from_tfds(in_memory_ds, info, split, batch_size,\n",
        "                           bert_preprocess_model):\n",
        "  is_training = split.startswith('train')\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
        "  num_examples = info.splits[split].num_examples\n",
        "\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(num_examples)\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  return dataset, num_examples"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-u1SwE51FML"
      },
      "source": [
        "Define your model\n",
        "\n",
        "You are now ready to define your model for sentence or sentence pair classification by feeding the preprocessed inputs through the BERT encoder and putting a linear classifier on top (or other arrangement of layers as you prefer), and using dropout for regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ahLcZ8NyYHL"
      },
      "source": [
        "def build_classifier_model(num_classes):\n",
        "  inputs = dict(\n",
        "      input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "      input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "  )\n",
        "\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
        "  net = encoder(inputs)['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
        "  net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(inputs, net, name='prediction')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkZDHjuZ1Kr6",
        "outputId": "4de6288c-f08d-46e5-e3d5-f3d16f8233f2"
      },
      "source": [
        "# Let's try running the model on some preprocessed inputs.\n",
        "\n",
        "test_classifier_model = build_classifier_model(2)\n",
        "bert_raw_result = test_classifier_model(text_preprocessed)\n",
        "print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.11534873 0.85651255]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "UVn9DclC1oCd",
        "outputId": "bdae937c-fd0c-4122-8053-dffe92feda35"
      },
      "source": [
        "# Let's take a look at the model's structure. You can see the three BERT expected inputs.\n",
        "tf.keras.utils.plot_model(test_classifier_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn7qZZMI22Ac"
      },
      "source": [
        "Choose a task from GLUE\n",
        "You are going to use a TensorFlow DataSet from the GLUE benchmark suite.\n",
        "\n",
        "Colab lets you download these small datasets to the local filesystem, and the code below reads them entirely into memory, because the separate TPU worker host cannot access the local filesystem of the colab runtime.\n",
        "\n",
        "For bigger datasets, you'll need to create your own Google Cloud Storage bucket and have the TPU worker read the data from there. You can learn more in the TPU guide.\n",
        "\n",
        "It's recommended to start with the CoLa dataset (for single sentence) or MRPC (for multi sentence) since these are small and don't take long to fine tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3BKQH8c103o",
        "outputId": "f349fef6-052b-4144-d8cb-a2b2020c7d46"
      },
      "source": [
        "tfds_name = 'glue/cola' \n",
        "# tfds_name = 'glue/sst2' \n",
        "# tfds_name = 'glue/qnli'\n",
        "\n",
        "tfds_info = tfds.builder(tfds_name).info\n",
        "\n",
        "sentence_features = list(tfds_info.features.keys())\n",
        "sentence_features.remove('idx')\n",
        "sentence_features.remove('label')\n",
        "\n",
        "available_splits = list(tfds_info.splits.keys())\n",
        "train_split = 'train'\n",
        "validation_split = 'validation'\n",
        "test_split = 'test'\n",
        "if tfds_name == 'glue/mnli':\n",
        "  validation_split = 'validation_matched'\n",
        "  test_split = 'test_matched'\n",
        "\n",
        "num_classes = tfds_info.features['label'].num_classes\n",
        "num_examples = tfds_info.splits.total_num_examples\n",
        "\n",
        "print(f'Using {tfds_name} from TFDS')\n",
        "print(f'This dataset has {num_examples} examples')\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f'Features {sentence_features}')\n",
        "print(f'Splits {available_splits}')\n",
        "\n",
        "with tf.device('/job:localhost'):\n",
        "  # batch_size=-1 is a way to load the dataset into memory\n",
        "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n",
        "\n",
        "# The code below is just to show some samples from the selected dataset\n",
        "print(f'Here are some sample rows from {tfds_name} dataset')\n",
        "sample_dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[train_split])\n",
        "\n",
        "labels_names = tfds_info.features['label'].names\n",
        "print(labels_names)\n",
        "print()\n",
        "\n",
        "sample_i = 1\n",
        "for sample_row in sample_dataset.take(5):\n",
        "  samples = [sample_row[feature] for feature in sentence_features]\n",
        "  print(f'sample row {sample_i}')\n",
        "  for sample in samples:\n",
        "    print(sample.numpy())\n",
        "  sample_label = sample_row['label']\n",
        "\n",
        "  print(f'label: {sample_label} ({labels_names[sample_label]})')\n",
        "  print()\n",
        "  sample_i += 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using glue/cola from TFDS\n",
            "This dataset has 10657 examples\n",
            "Number of classes: 2\n",
            "Features ['sentence']\n",
            "Splits ['train', 'validation', 'test']\n",
            "Here are some sample rows from glue/cola dataset\n",
            "['unacceptable', 'acceptable']\n",
            "\n",
            "sample row 1\n",
            "b'It is this hat that it is certain that he was wearing.'\n",
            "label: 1 (acceptable)\n",
            "\n",
            "sample row 2\n",
            "b'Her efficient looking up of the answer pleased the boss.'\n",
            "label: 1 (acceptable)\n",
            "\n",
            "sample row 3\n",
            "b'Both the workers will wear carnations.'\n",
            "label: 1 (acceptable)\n",
            "\n",
            "sample row 4\n",
            "b'John enjoyed drawing trees for his syntax homework.'\n",
            "label: 1 (acceptable)\n",
            "\n",
            "sample row 5\n",
            "b'We consider Leslie rather foolish, and Lou a complete idiot.'\n",
            "label: 1 (acceptable)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3hWUaHNAEhy"
      },
      "source": [
        "# The dataset also determines the problem type (classification or regression) and the appropriate loss function for training.\n",
        "def get_configuration(glue_task):\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  if glue_task is 'glue/cola':\n",
        "    metrics = tfa.metrics.MatthewsCorrelationCoefficient()\n",
        "  else:\n",
        "    metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        'accuracy', dtype=tf.float32)\n",
        "\n",
        "  return metrics, loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKWxV4r4BhtF"
      },
      "source": [
        "**Train your model**\n",
        "\n",
        "Finally, you can train the model end-to-end on the dataset you chose.\n",
        "\n",
        "**Distribution**\n",
        "\n",
        "Recall the set-up code at the top, which has connected the colab runtime to a TPU worker with multiple TPU devices. To distribute training onto them, you will create and compile your main Keras model within the scope of the TPU distribution strategy. (For details, see Distributed training with Keras.)\n",
        "\n",
        "Preprocessing, on the other hand, runs on the CPU of the worker host, not the TPUs, so the Keras model for preprocessing as well as the training and validation datasets mapped with it are built outside the distribution strategy scope. The call to Model.fit() will take care of distributing the passed-in dataset to the model replicas.\n",
        "\n",
        "Note: The single TPU worker host already has the resource objects (think: a lookup table) needed for tokenization. Scaling up to multiple workers requires use of Strategy.experimental_distribute_datasets_from_function with a function that loads the preprocessing model separately onto each worker.\n",
        "\n",
        "**Optimizer**\n",
        "\n",
        "Fine-tuning follows the optimizer set-up from BERT pre-training (as in Classify text with BERT): It uses the AdamW optimizer with a linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeN6pr1fAlZS",
        "outputId": "873efefd-63a3-4508-89d4-46ef498e1ed6"
      },
      "source": [
        "epochs = 3\n",
        "batch_size = 32\n",
        "init_lr = 2e-5\n",
        "\n",
        "print(f'Fine tuning {tfhub_handle_encoder} model for {tfds_name}')\n",
        "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "  # metric have to be created inside the strategy scope\n",
        "  metrics, loss = get_configuration(tfds_name)\n",
        "\n",
        "  train_dataset, train_data_size = load_dataset_from_tfds(\n",
        "      in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
        "  steps_per_epoch = train_data_size // batch_size\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "  validation_dataset, validation_data_size = load_dataset_from_tfds(\n",
        "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
        "      bert_preprocess_model)\n",
        "  validation_steps = validation_data_size // batch_size\n",
        "\n",
        "  classifier_model = build_classifier_model(num_classes)\n",
        "\n",
        "  optimizer = optimization.create_optimizer(\n",
        "      init_lr=init_lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      optimizer_type='adamw')\n",
        "\n",
        "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "\n",
        "  classifier_model.fit(\n",
        "      x=train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      validation_steps=validation_steps)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine tuning https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3 model for glue/cola\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['idx', 'label'] which did not match any model input. They will be ignored by the model.\n",
            "  [n for n in tensors.keys() if n not in ref_input_names])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"AdamWeightDecay/gradients/StatefulPartitionedCall:1\", shape=(None,), dtype=int32), values=Tensor(\"clip_by_global_norm/clip_by_global_norm/_0:0\", dtype=float32), dense_shape=Tensor(\"AdamWeightDecay/gradients/StatefulPartitionedCall:2\", shape=(None,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "267/267 [==============================] - 184s 226ms/step - loss: 0.6003 - accuracy: 0.7093 - val_loss: 0.5115 - val_accuracy: 0.8379\n",
            "Epoch 2/3\n",
            "267/267 [==============================] - 51s 192ms/step - loss: 0.3190 - accuracy: 0.8933 - val_loss: 0.5538 - val_accuracy: 0.8486\n",
            "Epoch 3/3\n",
            "267/267 [==============================] - 51s 192ms/step - loss: 0.1628 - accuracy: 0.9530 - val_loss: 0.6790 - val_accuracy: 0.8496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KJqwo-5e0NQ"
      },
      "source": [
        "# Results\n",
        "# CoLA 82.4% validation accuracy for model bert_en_uncased_L-12_H-768_A-12\n",
        "# 267/267 [==============================] - 21s 78ms/step - loss: 0.2550 - accuracy: 0.9114 - val_loss: 0.6072 - val_accuracy: 0.8242\n",
        "# CoLA 84.9% validation accuracy for model bert_en_wwm_uncased_L-24_H-1024_A-16/3\n",
        "267/267 [==============================] - 51s 192ms/step - loss: 0.1628 - accuracy: 0.9530 - val_loss: 0.6790 - val_accuracy: 0.8496\n",
        "\n",
        "# SST-2 92.5% validation accuracy for model bert_en_uncased_L-12_H-768_A-12\n",
        "# 2104/2104 [==============================] - 157s 75ms/step - loss: 0.0784 - accuracy: 0.9788 - val_loss: 0.3405 - val_accuracy: 0.9259\n",
        "# SST-2 93.6% validation accuracy for model bert_en_wwm_uncased_L-24_H-1024_A-16/3\n",
        "# 3273/3273 [==============================] - 624s 191ms/step - loss: 0.1069 - accuracy: 0.9731 - val_loss: 0.3246 - val_accuracy: 0.9331\n",
        "\n",
        "# QNLI 91.6% validation accuracy for model bert_en_uncased_L-12_H-768_A-12\n",
        "# 3273/3273 [==============================] - 252s 77ms/step - loss: 0.1556 - accuracy: 0.9546 - val_loss: 0.3349 - val_accuracy: 0.9158\n",
        "# QNLI 93.8% validation accuracy for model bert_en_wwm_uncased_L-24_H-1024_A-16/3\n",
        "# 3273/3273 [==============================] - 614s 188ms/step - loss: 0.0895 - accuracy: 0.9775 - val_loss: 0.3140 - val_accuracy: 0.9384\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhueflzzKfqF"
      },
      "source": [
        "**Export for inference**\n",
        "\n",
        "You will create a final model that has the preprocessing part and the fine-tuned BERT we've just created.\n",
        "\n",
        "At inference time, preprocessing needs to be part of the model (because there is no longer a separate input queue as for training data that does it). Preprocessing is not just computation; it has its own resources (the vocab table) that must be attached to the Keras Model that is saved for export. This final assembly is what will be saved.\n",
        "\n",
        "You are going to save the model on colab and later you can download to keep it for the future (View -> Table of contents -> Files)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFT1L3bEC9-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93721774-9755-42d8-a28a-29449277c6ac"
      },
      "source": [
        "main_save_path = './my_models'\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
        "\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
        "\n",
        "preprocess_inputs = bert_preprocess_model.inputs\n",
        "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
        "bert_outputs = classifier_model(bert_encoder_inputs)\n",
        "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
        "\n",
        "print(f'Saving {saved_model_path}')\n",
        "\n",
        "# Save everything on the Colab host (even the variables from TPU memory)\n",
        "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "model_for_export.save(saved_model_path, include_optimizer=False, options=save_options)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving ./my_models/glue_cola_bert_en_wwm_uncased_L-24_H-1024_A-16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1810). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 1810). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYn-V24ZKrVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb79989c-294a-41d9-f45c-d570723e5ffe"
      },
      "source": [
        "%ls my_models"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mglue_cola_bert_en_wwm_uncased_L-24_H-1024_A-16\u001b[0m/\n",
            "\u001b[01;34mglue_qnli_bert_en_wwm_uncased_L-24_H-1024_A-16\u001b[0m/\n",
            "\u001b[01;34mglue_sst2_bert_en_wwm_uncased_L-24_H-1024_A-16\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koI4H09JMFam"
      },
      "source": [
        "**Test the model**\n",
        "\n",
        "The final step is testing the results of your exported model.\n",
        "\n",
        "Just to make some comparison, let's reload the model and test it using some inputs from the test split from the dataset.\n",
        "\n",
        "Note: The test is done on the colab host, not the TPU worker that it has connected to, so it appears below with explicit device placements. You can omit those when loading the SavedModel elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfxnfMOjKxQm"
      },
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1IuPiz0MMGJ"
      },
      "source": [
        "def prepare(record):\n",
        "  model_inputs = [[record[ft]] for ft in sentence_features]\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "def prepare_serving(record):\n",
        "  model_inputs = {ft: record[ft] for ft in sentence_features}\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "def print_bert_results(test, bert_result, dataset_name):\n",
        "\n",
        "  bert_result_class = tf.argmax(bert_result, axis=1)[0]\n",
        "\n",
        "  if dataset_name == 'glue/cola':\n",
        "    print(f'sentence: {test[0].numpy()}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'This sentence is acceptable')\n",
        "    else:\n",
        "      print(f'This sentence is unacceptable')\n",
        "\n",
        "  elif dataset_name == 'glue/sst2':\n",
        "    print(f'sentence: {test[0]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'This sentence has POSITIVE sentiment')\n",
        "    else:\n",
        "      print(f'This sentence has NEGATIVE sentiment')\n",
        "\n",
        "  elif dataset_name == 'glue/mrpc':\n",
        "    print(f'sentence1: {test[0]}')\n",
        "    print(f'sentence2: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'Are a paraphrase')\n",
        "    else:\n",
        "      print(f'Are NOT a paraphrase')\n",
        "\n",
        "  elif dataset_name == 'glue/qqp':\n",
        "    print(f'question1: {test[0]}')\n",
        "    print(f'question2: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'Questions are similar')\n",
        "    else:\n",
        "      print(f'Questions are NOT similar')\n",
        "\n",
        "  elif dataset_name == 'glue/mnli':\n",
        "    print(f'premise   : {test[0]}')\n",
        "    print(f'hypothesis: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'This premise is NEUTRAL to the hypothesis')\n",
        "    elif bert_result_class == 2:\n",
        "      print(f'This premise CONTRADICTS the hypothesis')\n",
        "    else:\n",
        "      print(f'This premise ENTAILS the hypothesis')\n",
        "\n",
        "  elif dataset_name == 'glue/qnli':\n",
        "    print(f'question: {test[0]}')\n",
        "    print(f'sentence: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'The question is NOT answerable by the sentence')\n",
        "    else:\n",
        "      print(f'The question is answerable by the sentence')\n",
        "\n",
        "  elif dataset_name == 'glue/rte':\n",
        "    print(f'sentence1: {test[0]}')\n",
        "    print(f'sentence2: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'Sentence1 DOES NOT entails sentence2')\n",
        "    else:\n",
        "      print(f'Sentence1 entails sentence2')\n",
        "\n",
        "  elif dataset_name == 'glue/wnli':\n",
        "    print(f'sentence1: {test[0]}')\n",
        "    print(f'sentence2: {test[1]}')\n",
        "    if bert_result_class == 1:\n",
        "      print(f'Sentence1 DOES NOT entails sentence2')\n",
        "    else:\n",
        "      print(f'Sentence1 entails sentence2')\n",
        "\n",
        "  print(f'Bert raw results:{bert_result[0]}')\n",
        "  print()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKr1XxpgMr_4"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzK8ukxXMcy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ec4568-2502-449f-a396-e0eee5587e0c"
      },
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[test_split])\n",
        "  for test_row in test_dataset.shuffle(1000).map(prepare).take(5):\n",
        "    if len(sentence_features) == 1:\n",
        "      result = reloaded_model(test_row[0])\n",
        "    else:\n",
        "      result = reloaded_model(list(test_row))\n",
        "\n",
        "    print_bert_results(test_row, result, tfds_name)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence: [b'That silly fool broke the teapot.']\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-3.0747712  4.318989 ]\n",
            "\n",
            "sentence: [b'Steve tossed Anna the ball.']\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-2.2994602  3.3863993]\n",
            "\n",
            "sentence: [b'The book was given by John to Mary.']\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-2.9519103  4.103157 ]\n",
            "\n",
            "sentence: [b'Kevin urged Anne to be loyal to herself.']\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-3.7937217  4.264505 ]\n",
            "\n",
            "sentence: [b\"When we sell the house, we'll probably leave most of the furniture.\"]\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-3.0671856  3.8671496]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8xF1nhf9MGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a08fca-41cb-4f98-e5d2-1673306f4e03"
      },
      "source": [
        "test_dataset.element_spec"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idx': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
              " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
              " 'sentence': TensorSpec(shape=(), dtype=tf.string, name=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PQPWeNsM98o"
      },
      "source": [
        "If you want to use your model on TF Serving, remember that it will call your SavedModel through one of its named signatures. Notice there are some small differences in the input. In Python, you can test them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blx9rsQHMvnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c9ffab-7a97-4581-a09e-d7f0f3452da4"
      },
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  serving_model = reloaded_model.signatures['serving_default']\n",
        "  for test_row in test_dataset.shuffle(1000).map(prepare_serving).take(5):\n",
        "    result = serving_model(**test_row)\n",
        "    # The 'prediction' key is the classifier's defined model name.\n",
        "    print_bert_results(list(test_row.values()), result['prediction'], tfds_name)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence: b'She thought it was likely that everyone would fit into the car.'\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-2.668784   4.0301905]\n",
            "\n",
            "sentence: b'John made a doll for his son.'\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-3.8181999  4.2870173]\n",
            "\n",
            "sentence: b'Sentences some go on and on.'\n",
            "This sentence is unacceptable\n",
            "Bert raw results:[ 1.2281017 -2.8270729]\n",
            "\n",
            "sentence: b'She laughed in embarrassment.'\n",
            "This sentence is acceptable\n",
            "Bert raw results:[-3.237706  4.2011  ]\n",
            "\n",
            "sentence: b'We persuaded there to be several students at the talk.'\n",
            "This sentence is unacceptable\n",
            "Bert raw results:[ 1.6691458 -2.952706 ]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcMf3oMxNAa_"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}