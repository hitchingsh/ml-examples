{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_Translation_Using_the_T5_Model_And_HuggingFace_Framework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXGZC40ZzV0x"
      },
      "source": [
        "# Language Translation Using the T5 and MarianMT Models and HuggingFace Framework\n",
        "\n",
        "Translates English to German with high accuracy in only a few lines of code by using the HuggingFace Framework. \n",
        "\n",
        "This notebook shows both the highest level abstraction with pipelines and slightly lower level APIs where we create the model and tokenizer before doing the translations.\n",
        "\n",
        "This example use the same test sentences I used in my 2nd Coursera class capstone project plus a bonus complex sentence.  \n",
        "\n",
        "Uses the T5 Model (Text-to-Text Transfer Transformer) from Google https://arxiv.org/abs/1910.10683\n",
        "\n",
        "The T5 Model uses the C4 dataset (Colossal Clean Crawled Corpus) consisting of about 750 gigabytes of clean English text scraped from the web\n",
        "\n",
        "T5 has been fine tuned to a number of specific NLP tasks including those in the GLUE and SuperGlue NLP benchmarks, which can be run using the code shown below with minor modifications.\n",
        "\n",
        "Also, demonstrate translating the same English to German using the MarianMT model, which is pretrained on over 1,000 language translation combinations so easy to generalize.\n",
        "\n",
        "https://huggingface.co/transformers/master/model_doc/marian.html \n",
        "\n",
        "Lastly translate the same strings via the Google API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMa9Z09E0x_-"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3XGY_30zrHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c552e7f-fec9-4a57-fce6-1dc5f99c0357"
      },
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at t5-base were not used when initializing T5Model: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldT_pwIC23Qd"
      },
      "source": [
        "english_strings = [\"I need my key.\", \"I have won.\", \"take a bus\", \"Do you know that?\", \"That'll be fun.\", \"You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luPpQ4Jnz5XZ",
        "outputId": "60318bda-9f84-4c07-d701-9c9389aeaefa"
      },
      "source": [
        "for english_string in english_strings:\n",
        "  print(f'English: {english_string}')\n",
        "  german_string = translator(english_string)[0]['translation_text']\n",
        "  print(f'German: {german_string}\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: I need my key.\n",
            "German: Ich brauche meinen Schlüssel.\n",
            "\n",
            "English: I have won.\n",
            "German: Ich habe gewonnen.\n",
            "\n",
            "English: take a bus\n",
            "German: Bus nehmen\n",
            "\n",
            "English: Do you know that?\n",
            "German: Wissen Sie das?\n",
            "\n",
            "English: That'll be fun.\n",
            "German: Das wird Spaß machen.\n",
            "\n",
            "English: You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\n",
            "German: Sie können mit United Airlines von San Francisco nach München fliegen, aber es ist ein langer Flug.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4XZu3Cy118M"
      },
      "source": [
        "Going one level lower in the HuggingFace API in case we needed more control including the ability to specify the model, tokenizer and if needed the config.  Note, we are using T5 summarization and a special feature to do translation. \n",
        "\n",
        "The following uses PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RTl0S8y03G0",
        "outputId": "8d324b4c-7739-49f6-b679-11539b4c95e6"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:890: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OXwieh62i5J",
        "outputId": "c7b19c97-afc0-41d2-cca8-86ddadf7e415"
      },
      "source": [
        "for english_string in english_strings:\n",
        "  print(f'English: {english_string}')\n",
        "\n",
        "  # Add the T5 specific prefix “translate English to German: “ since this is a generative model\n",
        "  # Other prefixes are available in T5 for other NLP tasks including those in GLUE & SuperGLUE\n",
        "  english_string = \"translate English to German:\" + english_string\n",
        "\n",
        "  inputs = tokenizer.encode(english_string, return_tensors=\"pt\")\n",
        "\n",
        "  # Overriding PreTrainedModel.generate() default config, e.g. max_length\n",
        "  outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "  german_string = tokenizer.decode(outputs[0]).lstrip('<pad>').rstrip('</s>')\n",
        "  print(f'German: {german_string}\\n')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: I need my key.\n",
            "German:  Ich brauche meinen Schlüssel.\n",
            "\n",
            "English: I have won.\n",
            "German:  Ich habe gewonnen.\n",
            "\n",
            "English: take a bus\n",
            "German:  Bus nehmen\n",
            "\n",
            "English: Do you know that?\n",
            "German:  Wissen Sie das?\n",
            "\n",
            "English: That'll be fun.\n",
            "German:  Das wird Spaß machen.\n",
            "\n",
            "English: You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\n",
            "German:  Sie können mit United Airlines von San Francisco nach München fliegen, aber es ist ein langer Flug.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JAB7UU28UOr"
      },
      "source": [
        "Same as above except in Tensorflow 2.0.  Notice the differences are trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YONwHkLs5uPw",
        "outputId": "ec37ad76-bc82-4839-afe2-578d5f47d682"
      },
      "source": [
        "from transformers import TFAutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_tf_auto.py:776: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYaBZSZ_8jtZ",
        "outputId": "c539ba50-fdce-42cd-f652-da4062792286"
      },
      "source": [
        "for english_string in english_strings:\n",
        "  print(f'English: {english_string}')\n",
        "\n",
        "  # Add the T5 specific prefix “translate English to German: “ since this is a generative model\n",
        "  # Other prefixes are available in T5 for other NLP tasks including those in GLUE & SuperGLUE\n",
        "  english_string = \"translate English to German:\" + english_string\n",
        "\n",
        "  inputs = tokenizer.encode(english_string, return_tensors=\"tf\")\n",
        "\n",
        "  # Overriding PreTrainedModel.generate() default config, e.g. max_length\n",
        "  outputs = model.generate(inputs, max_length=50, num_beams=4, early_stopping=True)\n",
        "  german_string = tokenizer.decode(outputs[0]).lstrip('<pad>').rstrip('</s>')\n",
        "  print(f'German: {german_string}\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: I need my key.\n",
            "German:  Ich brauche meinen Schlüssel.\n",
            "\n",
            "English: I have won.\n",
            "German:  Ich habe gewonnen.\n",
            "\n",
            "English: take a bus\n",
            "German:  Bus nehmen\n",
            "\n",
            "English: Do you know that?\n",
            "German:  Wissen Sie das?\n",
            "\n",
            "English: That'll be fun.\n",
            "German:  Das wird Spaß machen.\n",
            "\n",
            "English: You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\n",
            "German:  Sie können mit United Airlines von San Francisco nach München fliegen, aber es ist ein langer Flug.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEL01LtglaXQ"
      },
      "source": [
        "**MarianMT Model**\n",
        "\n",
        "Over 1000 language combinations supported. German translation below had one sentence not translated as well as T5.\n",
        "\n",
        "https://huggingface.co/transformers/master/model_doc/marian.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDkhEeStlt4d",
        "outputId": "48c9ed87-6cea-4e9e-e581-071359065e76"
      },
      "source": [
        "% pip install sentencepiece"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmc8kFxJlzaU",
        "outputId": "761087b8-9574-4421-d465-3c9fb366bb5d"
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "src_text = [\">>deu<< \" + s for s in english_strings]\n",
        "\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-de'\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "print(tokenizer.supported_language_codes)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "outputs = model.generate(**tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
        "german_strings = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of MarianMTModel were not initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-de and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pvnx3ZXqUNc",
        "outputId": "4a69cf27-5470-4304-ad7c-851ad8f61197"
      },
      "source": [
        "for english_string, german_string in zip(english_strings, german_strings):\n",
        "  print(f'English: {english_string}')\n",
        "  print(f'German: {german_string}\\n')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: I need my key.\n",
            "German: Ich brauche meinen Schlüssel.\n",
            "\n",
            "English: I have won.\n",
            "German: Ich habe gewonnen.\n",
            "\n",
            "English: take a bus\n",
            "German: Nehmen Sie einen Bus\n",
            "\n",
            "English: Do you know that?\n",
            "German: Weißt du das?\n",
            "\n",
            "English: That'll be fun.\n",
            "German: Das wird lustig.\n",
            "\n",
            "English: You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\n",
            "German: Sie können nonstop von San Francisco nach München auf United Airlines fliegen, aber es ist ein langer Flug\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHh05K2i6wBU"
      },
      "source": [
        "**Google Translate API**\n",
        "\n",
        "Use a modified/patched version of Google Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrRIsqZj60H6",
        "outputId": "1e22434f-5c35-41e4-f863-6f6d9551fc9a"
      },
      "source": [
        "! pip install google_trans_new"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.6/dist-packages (1.1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96d_Ojmc68Q7"
      },
      "source": [
        "from google_trans_new import google_translator\n",
        "\n",
        "translator = google_translator()  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdwix5oE7I0b",
        "outputId": "d0faa9b9-08e5-454f-b7b4-6cc7c0b21c8d"
      },
      "source": [
        "for english_string in english_strings:\n",
        "  german_string = translator.translate(english_string,lang_tgt='de')  \n",
        "  print(f'English: {english_string}')\n",
        "  print(f'German: {german_string}\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: I need my key.\n",
            "German: Ich brauche meinen Schlüssel. \n",
            "\n",
            "English: I have won.\n",
            "German: Ich habe gewonnen. \n",
            "\n",
            "English: take a bus\n",
            "German: nehmen Sie einen Bus \n",
            "\n",
            "English: Do you know that?\n",
            "German: Weißt du, dass? \n",
            "\n",
            "English: That'll be fun.\n",
            "German: Das wird Spaß machen. \n",
            "\n",
            "English: You can fly non-stop from San Francisco to Munich on United Airlines but its a long flight\n",
            "German: Sie können mit United Airlines nonstop von San Francisco nach München fliegen, aber es ist ein langer Flug \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}