{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_Answering_Models_BERT_Roberta_Electra_Pretrained_on_Squad2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbckpmiboDTt"
      },
      "source": [
        "pip install -q transformers"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIhcxdhJhMev"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qLVt3FKSSm6"
      },
      "source": [
        "context1 = r\"\"\"ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "    architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "    Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "    TensorFlow 2.0 and PyTorch.\"\"\"\n",
        "\n",
        "contexts = [\n",
        "\n",
        "r\"\"\"The four largest cities in the Netherlands are Amsterdam, Rotterdam, The Hague and Utrecht.[17] Amsterdam is the country's most populous city \n",
        " and nominal capital,[18] while The Hague holds the seat of the States General, Cabinet and Supreme Court.[19] The Port of Rotterdam is the busiest \n",
        " seaport in Europe, and the busiest in any country outside East Asia and Southeast Asia, behind only China and Singapore.\"\"\",\n",
        "r\"\"\"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        " question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        " a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\"\"\",\n",
        "\"The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\",\n",
        "r\"\"\"MOSCOW, Russia (CNN) â€“ Russian space officials say the crew of the Soyuz space ship is resting after a rough ride back to Earth.\"\n",
        "  A South Korean bioengineer was one of three people on board the Soyuz capsule. The craft carrying South Koreaâ€™s first astronaut \n",
        "  landed in northern Kazakhstan on Saturday, 260 miles (418 kilometers) off its mark, they said. Mission Control spokesman Valery \n",
        "  Lyndin said the condition of the crew â€“ South Korean bioengineer Yi So-yeon, American astronaut Peggy Whitson and Russian flight \n",
        "  engineer Yuri Malenchenko â€“ was satisfactory, though the three had been subjected to severe G-forces during the re-entry.\"\"\",\n",
        "context1, context1\n",
        "]\n",
        "\n",
        "questions = [\n",
        "\"What is the capital of the Netherlands?\",\n",
        "\"What is a good example of a question answering dataset?\",\n",
        "\"Why is model conversion important?\",\n",
        "\"Where did the Soyuz capsule land?\",\n",
        "\"How many pretrained models are available in ðŸ¤— Transformers?\",\n",
        "\"ðŸ¤— Transformers provides interoperability between which frameworks?\"\n",
        "]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abObwmFrf_CP"
      },
      "source": [
        "model_names = [\"deepset/roberta-base-squad2\", \"bert-large-uncased-whole-word-masking-finetuned-squad\", \\\n",
        "               \"ahotrod/electra_large_discriminator_squad2_512\"]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaZEJq1V4puz"
      },
      "source": [
        "# Suppress irrelevant warning\n",
        "\n",
        "# To control logging level for various modules used in the application:\n",
        "import logging\n",
        "import re\n",
        "def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n",
        "    \"\"\"\n",
        "    Override logging levels of different modules based on their name as a prefix.\n",
        "    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n",
        "\n",
        "    Args:\n",
        "        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n",
        "        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n",
        "          Default is `[\"\"]` to match all active loggers.\n",
        "          The match is a case-sensitive `module_name.startswith(prefix)`\n",
        "    \"\"\"\n",
        "    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n",
        "    for name in logging.root.manager.loggerDict:\n",
        "        if re.match(prefix_re, name):\n",
        "            logging.getLogger(name).setLevel(level)\n",
        "\n",
        "set_global_logging_level(logging.ERROR)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VtEUiYvE339",
        "outputId": "8df485da-1895-4527-9d2b-ddcf5ee8af13"
      },
      "source": [
        "print(f\"Predicting Answers to {len(questions)} Questions using {len(model_names)} different models...\\n\")\n",
        "\n",
        "predicted_answers = [[0 for x in range(len(model_names))] for y in range(len(questions))] \n",
        "predicted_scores = [[0 for x in range(len(model_names))] for y in range(len(questions))] \n",
        "\n",
        "for model_index in range(len(model_names)):\n",
        "    # Retrieve the pre-trained model. Slow.\n",
        "    nlp = pipeline('question-answering', model=model_names[model_index], tokenizer=model_names[model_index])\n",
        "\n",
        "    question_index = 0\n",
        "    for question, context in zip(questions, contexts):\n",
        "        # Predict the answer\n",
        "        answer = nlp(question=question, context=context)\n",
        "        predicted_answers[question_index][model_index] = answer['answer']\n",
        "        predicted_scores[question_index][model_index] = answer['score']\n",
        "        question_index += 1\n",
        "\n",
        "for question_index in range(0, len(questions)):\n",
        "    print(f'Question: {questions[question_index]}')\n",
        "    for model_index in range(0, len(model_names)):\n",
        "        print(f\"Model: {model_names[model_index]} Answer: '{predicted_answers[question_index][model_index]}'\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting Answers to 6 Questions using 3 different models...\n",
            "\n",
            "Question: What is the capital of the Netherlands?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'Amsterdam'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'Amsterdam'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: 'Amsterdam'\n",
            "\n",
            "\n",
            "Question: What is a good example of a question answering dataset?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'SQuAD dataset'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'SQuAD dataset'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: 'SQuAD dataset'\n",
            "\n",
            "\n",
            "Question: Why is model conversion important?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'gives freedom to the user'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'gives freedom to the user'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: 'gives freedom to the user and let people easily switch between frameworks'\n",
            "\n",
            "\n",
            "Question: Where did the Soyuz capsule land?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'northern Kazakhstan'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'northern Kazakhstan'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: 'northern Kazakhstan'\n",
            "\n",
            "\n",
            "Question: How many pretrained models are available in ðŸ¤— Transformers?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'over 32+'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'over 32+'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: '32+'\n",
            "\n",
            "\n",
            "Question: ðŸ¤— Transformers provides interoperability between which frameworks?\n",
            "Model: deepset/roberta-base-squad2 Answer: 'TensorFlow 2.0 and PyTorch'\n",
            "Model: bert-large-uncased-whole-word-masking-finetuned-squad Answer: 'TensorFlow 2.0 and PyTorch'\n",
            "Model: ahotrod/electra_large_discriminator_squad2_512 Answer: 'TensorFlow 2.0 and PyTorch'\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7HwJJKqAGP"
      },
      "source": [
        "Slightly lower level Huggingface implementation\n",
        "\n",
        "Some good explanations can be found here: https://towardsdatascience.com/question-and-answering-with-bert-6ef89a78dac"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNQnAnD9t9vV",
        "outputId": "c72cd5e8-c777-44c9-e367-8e2d9e5d28e2"
      },
      "source": [
        "print(\"Predicting answer using Electra model pretrained on squad2\\n\")\n",
        "\n",
        "import torch\n",
        "model_name = 'ahotrod/electra_large_discriminator_squad2_512'\n",
        "# b) Load model & tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(questions[0], contexts[0], add_special_tokens=True, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"].tolist()[0] # the list of all indices of words in question + context\n",
        "    \n",
        "text_tokens = tokenizer.convert_ids_to_tokens(input_ids) # Get the tokens for the question + context\n",
        "answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
        "\n",
        "answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "    \n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    \n",
        "print(f\"Question: {questions[0]}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting answer using Electra model pretrained on squad2\n",
            "\n",
            "Question: What is the capital of the Netherlands?\n",
            "Answer: amsterdam\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}